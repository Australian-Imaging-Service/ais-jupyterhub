# # JupyterHub Configuration for XNAT Integration (Hub 5.4.1 / Z2JH 4.3.x)
# # ==========================================================
# # - Internal culling: 2 hours idle timeout (aggressive, pod-level)
# # - External culling: 24 hours idle timeout (backup, hub-level)
# # ===============================================================================================================================

hub:
  baseUrl: /jupyter
  config:
    JupyterHub:
      authenticator_class: generic-oauth
      base_url: /jupyter
      log_level: INFO

    GenericOAuthenticator:
      client_id: "70870f7a-f70e-4acd-829f-0f611f1d40de"
      client_secret: "wdNMMmFGmufaMsP6txVEolL9vbNMyuZ0CFQuaxzpdZGqKFk27QquTQ6FOGChH9GmNXx4Vkmwkok2"
      oauth_callback_url: "https://xnat-test.ssdsorg.cloud.edu.au/jupyter/hub/oauth_callback"
      authorize_url: "https://central.test.aaf.edu.au/providers/op/authorize"
      token_url: "https://central.test.aaf.edu.au/providers/op/token"
      userdata_url: "https://central.test.aaf.edu.au/providers/op/userinfo"
      login_service: "AAF"
      username_claim: "sub"
      scope: [openid, profile, email, eduperson_principal_name, eduperson_scoped_affiliation, eduperson_affiliation, eduperson_assurance, schac_home_organization, aueduperson]
      allow_all: true
    
    Authenticator:
      enable_auth_state: false
      refresh_pre_spawn: false
    
    KubeSpawner:
      # Increase timeout for large image pulls (neurodesk images can be several GB)
      start_timeout: 900  # 15 minutes to allow for large image pulls
      http_timeout: 120   # 2 minutes for HTTP requests

      extra_resource_limits:
        smarter-devices/fuse: "1"
      extra_resource_guarantees:
        smarter-devices/fuse: "1"

      # Apply AppArmor profile to all notebooks automatically
      extra_container_config:
        securityContext:
          privileged: false
          # Uncomment below when AppArmor profile is loaded on nodes
          appArmorProfile:
            type: Localhost
            localhostProfile: notebook
  
  loadRoles:
    user-cull:
      scopes:
        - list:users
        - read:users:activity
        - read:servers
        - delete:servers
        - admin:users
      services:
        - user-cull
  
  services:
    xnat-service:
      admin: true
      apiToken: "eda886c63564930a7f21ad8465463bf9555bad7d1dfa0a2fb259ac556ea8420e"
    
    user-cull:
      command:
        - python3
        - -m
        - jupyterhub_idle_culler
        - --timeout=86400
        - --cull-every=3600
        - --cull-users
        - --url=http://localhost:8081/jupyter/hub/api

  extraEnv:
    JUPYTERHUB_CRYPT_KEY_HEX: "62c5252a18d4c48ea3b6ca199f501c439a58be3521d01f0055518a1d2a46c3fe"
    XNAT_HOST: "xnat-web.ais-xnat.svc.cluster.local"
    XNAT_USER: "admin"
    XNAT_PASSWORD: "admin"

  extraConfig:
    00_set_crypt_key: |
      import os, base64, binascii
      hexkey = os.environ.get("JUPYTERHUB_CRYPT_KEY_HEX", "").strip()
      if not hexkey:
        raise SystemExit("JUPYTERHUB_CRYPT_KEY_HEX missing")
      try:
        raw = binascii.unhexlify(hexkey)
      except binascii.Error as e:
        raise SystemExit(f"Invalid hex key: {e}")
      fernet_key = base64.urlsafe_b64encode(raw)
      c.CryptKeeper.keys = [fernet_key]

    01_add_xnat_username_column: |
      from sqlalchemy import Column, String
      from jupyterhub import orm

      if not hasattr(orm.User, 'xnat_username'):
          orm.User.xnat_username = Column(String(255), nullable=True)

      if not hasattr(orm.User, 'eduperson_principal_name'):
          orm.User.eduperson_principal_name = Column(String(255), nullable=True)

    02_username_normalization: |
      from oauthenticator.generic import GenericOAuthenticator
      from jupyterhub.apihandlers.users import UserAPIHandler, UserServerAPIHandler
      from jupyterhub.handlers import BaseHandler
      from jupyterhub import orm
      import json

      def normalize_username(username):
          if not username:
              return username, username

          original = username

          if username.startswith('aaf_'):
              username = username[4:]

          normalized = f'aaf_{username.lower()}'

          return normalized, original

      def normalize_aaf(self, username):
          normalized, original = normalize_username(username)
          return normalized

      GenericOAuthenticator.normalize_username = normalize_aaf

      original_authenticate = GenericOAuthenticator.authenticate

      async def authenticate_with_storage(self, handler, data=None, **kwargs):
          result = await original_authenticate(self, handler, data, **kwargs)

          if result and isinstance(result, dict):
              username = result.get('name')
              if username:
                  auth_state = result.get('auth_state', {})
                  user_info = auth_state.get('oauth_user', {})

                  if user_info:
                      original_username = user_info.get('sub', username)
                      eduperson_principal = user_info.get('edu_person_principal_name', '')

                      user = self.db.query(orm.User).filter_by(name=username).first()
                      if user:
                          if not user.xnat_username:
                              user.xnat_username = original_username
                          if not user.eduperson_principal_name and eduperson_principal:
                              user.eduperson_principal_name = eduperson_principal
                          self.db.commit()
                  else:
                      self.log.warning(f"AAF did not return user info for {username}")

          return result

      GenericOAuthenticator.authenticate = authenticate_with_storage
      
      original_user_get = UserAPIHandler.get
      original_user_post = UserAPIHandler.post
      original_user_delete = UserAPIHandler.delete
      original_user_patch = UserAPIHandler.patch
      
      async def user_get_wrapper(self, user_name):
          normalized, original = normalize_username(user_name)
          return await original_user_get(self, normalized)
      
      async def user_post_wrapper(self, user_name):
          normalized, original = normalize_username(user_name)
          result = await original_user_post(self, normalized)
          
          user = self.find_user(normalized)
          if user and not user.xnat_username:
              user.xnat_username = original
              self.db.commit()
          
          return result
      
      async def user_delete_wrapper(self, user_name):
          normalized, original = normalize_username(user_name)
          return await original_user_delete(self, normalized)
      
      async def user_patch_wrapper(self, user_name):
          normalized, original = normalize_username(user_name)
          return await original_user_patch(self, normalized)
      
      UserAPIHandler.get = user_get_wrapper
      UserAPIHandler.post = user_post_wrapper
      UserAPIHandler.delete = user_delete_wrapper
      UserAPIHandler.patch = user_patch_wrapper
      
      original_server_post = UserServerAPIHandler.post
      original_server_delete = UserServerAPIHandler.delete
      
      async def server_post_wrapper(self, user_name, server_name=''):
          normalized, original = normalize_username(user_name)
          
          user = self.find_user(normalized)
          if user and not user.xnat_username:
              user.xnat_username = original
              self.db.commit()
          
          return await original_server_post(self, normalized, server_name)
      
      async def server_delete_wrapper(self, user_name, server_name=''):
          normalized, original = normalize_username(user_name)
          return await original_server_delete(self, normalized, server_name)
      
      UserServerAPIHandler.post = server_post_wrapper
      UserServerAPIHandler.delete = server_delete_wrapper

    03_xnat_pre_spawn_hook: |
      import json

      async def pre_spawn_hook(spawner):
          import requests
          from requests.auth import HTTPBasicAuth
          import os

          xnat_username = spawner.user.xnat_username or spawner.user.name

          existing_volumes = getattr(spawner, 'volumes', {})
          existing_mounts = getattr(spawner, 'volume_mounts', {})

          if isinstance(existing_volumes, dict):
              existing_volumes = list(existing_volumes.values())
          if isinstance(existing_mounts, dict):
              existing_mounts = list(existing_mounts.values())

          if not next((v for v in existing_volumes if v.get('name') == 'cvmfs'), None):
              spawner.log.error("CVMFS volume not found in configuration")

          xnat_volumes = [
              {'name': 'xnat-gpfs', 'persistentVolumeClaim': {'claimName': 'xnat-gpfs'}},
              {'name': 'shm', 'emptyDir': {'medium': 'Memory', 'sizeLimit': '2Gi'}}
          ]

          xnat_mounts = [{'name': 'shm', 'mountPath': '/dev/shm'}]

          spawner.volumes = existing_volumes + xnat_volumes
          spawner.volume_mounts = existing_mounts + xnat_mounts

          init_script = f"""
          set -e
          echo "[INIT] Starting workspace setup..."
          mkdir -p /data/xnat/workspaces/users

          if [ -d "/data/xnat/workspaces/users/{xnat_username}" ]; then
              echo "[INIT] Found XNAT workspace: {xnat_username}"
              if [ "{xnat_username}" != "{spawner.user.name}" ]; then
                  if [ ! -e "/data/xnat/workspaces/users/{spawner.user.name}" ]; then
                      ln -s "{xnat_username}" "/data/xnat/workspaces/users/{spawner.user.name}"
                      echo "[INIT] Created symlink: {spawner.user.name} -> {xnat_username}"
                  fi
              fi
          else
              mkdir -p "/data/xnat/workspaces/users/{spawner.user.name}"
              chown 1000:100 "/data/xnat/workspaces/users/{spawner.user.name}"
              chmod 755 "/data/xnat/workspaces/users/{spawner.user.name}"
              echo "[INIT] Created new workspace: {spawner.user.name}"
          fi

          echo "[INIT] Workspace setup complete"
          """
          
          spawner.init_containers = [
              {
                  'name': 'workspace-setup',
                  'image': 'busybox:1.35',
                  'command': ['sh', '-c'],
                  'args': [init_script],
                  'volumeMounts': [{'name': 'xnat-gpfs', 'mountPath': '/data/xnat'}],
                  'securityContext': {'runAsUser': 0}
              }
          ]

          workspace_mounts = [
              {
                  'name': 'xnat-gpfs',
                  'mountPath': f'/data/xnat/workspaces/users/{spawner.user.name}',
                  'subPath': f'workspaces/users/{spawner.user.name}',
                  'readOnly': False
              },
              {
                  'name': 'xnat-gpfs',
                  'mountPath': f'/workspace/{xnat_username}',
                  'subPath': f'workspaces/users/{spawner.user.name}',
                  'readOnly': False
              }
          ]

          spawner.volume_mounts.extend(workspace_mounts)
          spawner.working_dir = f'/workspace/{xnat_username}'

          try:
              xnat_host = os.getenv('XNAT_HOST', 'xnat-web.ais-xnat.svc.cluster.local')
              xnat_user = os.getenv('XNAT_USER', 'admin')
              xnat_pass = os.getenv('XNAT_PASSWORD', 'admin')

              response = requests.get(
                  f"http://{xnat_host}/xapi/jupyterhub/users/{xnat_username}/server/user-options",
                  auth=HTTPBasicAuth(xnat_user, xnat_pass),
                  timeout=5
              )

              if response.status_code == 200:
                  options = response.json()
                  task_template = options.get('task_template', {})

                  placement = task_template.get('placement', {})
                  constraints = placement.get('constraints', [])
                  if constraints:
                      spawner.node_selector = {i.split('==')[0]: i.split('==')[-1] for i in constraints}

                  resources = task_template.get('resources', {})
                  if resources.get('cpu_limit'):
                      spawner.cpu_limit = resources['cpu_limit']
                  if resources.get('cpu_reservation'):
                      spawner.cpu_guarantee = resources['cpu_reservation']
                  if resources.get('mem_limit'):
                      spawner.mem_limit = resources['mem_limit']
                  if resources.get('mem_reservation'):
                      spawner.mem_guarantee = resources['mem_reservation']

                  generic_resources = resources.get('generic_resources', {})
                  if generic_resources:
                      if 'gpu' in generic_resources:
                          generic_resources['nvidia.com/gpu'] = generic_resources.pop('gpu')
                      spawner.extra_resource_guarantees = generic_resources
                      spawner.extra_resource_limits = generic_resources

                  container_spec = task_template.get('container_spec', {})
                  image = container_spec.get('image')
                  if image:
                      spawner.image = image
                      spawner.image_pull_policy = 'IfNotPresent'

                  command = container_spec.get('command')
                  if command:
                      spawner.cmd = command.split(' ')

                  env = container_spec.get('env', {})
                  filtered_env = {
                      k: v for k, v in env.items()
                      if k not in ['JUPYTERHUB_ROOT_DIR', 'XDG_CONFIG_HOME']
                  }
                  spawner.environment.update(filtered_env)

                  mounts = container_spec.get('mounts', [])
                  for mount in mounts:
                      source = mount.get('source', '')
                      target = mount.get('target', '')
                      read_only = mount.get('read_only', True)

                      if 'workspaces' in source or not source.startswith('/data/xnat/'):
                          continue

                      subpath = source[len('/data/xnat/'):]
                      xnat_mount = {
                          'name': 'xnat-gpfs',
                          'mountPath': target,
                          'subPath': subpath,
                          'readOnly': read_only
                      }
                      spawner.volume_mounts.append(xnat_mount)
              else:
                  spawner.log.warning(f"XNAT API returned status {response.status_code}")

          except Exception as e:
              spawner.log.warning(f"XNAT API error: {e}")
      
      c.KubeSpawner.pre_spawn_hook = pre_spawn_hook

  db:
    type: sqlite-pvc
    pvc:
      storageClassName: longhorn
      storage: 5Gi

proxy:
  service:
    type: ClusterIP

singleuser:
  defaultUrl: /lab

  # Reduce termination grace period to prevent stuck pods
  extraPodConfig:
    terminationGracePeriodSeconds: 10

  extraFiles:
    internal_culling_config:
      mountPath: /etc/jupyter/jupyter_server_config.py
      stringData: |
        c.ServerApp.shutdown_no_activity_timeout = 7200
        c.MappingKernelManager.cull_idle_timeout = 5400
        c.MappingKernelManager.cull_interval = 600
        c.MappingKernelManager.cull_connected = True
        c.MappingKernelManager.cull_busy = False
        c.TerminalManager.cull_inactive_timeout = 7200
        c.TerminalManager.cull_interval = 600
        
        import logging
        logging.getLogger('ServerApp').setLevel(logging.INFO)

  image:
    name: ghcr.io/neurodesk/neurodesktop/neurodesktop
    tag: "2025-09-23"
    pullPolicy: IfNotPresent

  # Resource limits and guarantees
  cpu:
    guarantee: 1
    limit: 4
  memory:
    guarantee: 1G
    limit: 8G

  # Storage configuration with volumes
  storage:
    type: dynamic
    dynamic:
      storageClass: longhorn
      pvcNameTemplate: "jupyter-{username}"
      volumeNameTemplate: "jupyter-{username}"
      storageAccessModes: [ReadWriteOnce]
    capacity: 10Gi
    homeMountPath: /home/jovyan
    
    # Extra volumes under storage
    extraVolumes:
      - name: cvmfs
        persistentVolumeClaim:
          claimName: cvmfs
    
    extraVolumeMounts:
      - name: cvmfs
        mountPath: /cvmfs
        mountPropagation: HostToContainer

  # Environment variables
  extraEnv:
    JUPYTER_ENABLE_LAB: "yes"

ingress:
  enabled: true
  ingressClassName: nginx
  annotations:
    nginx.ingress.kubernetes.io/proxy-body-size: "0"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "600"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "600"
  hosts:
    - xnat-test.ssdsorg.cloud.edu.au
  pathType: Prefix
  tls: []

scheduling:
  userScheduler: { enabled: false }
  podPriority: { enabled: false }
  userPlaceholder: { enabled: false }

prePuller:
  hook: { enabled: false }
  continuous: { enabled: false }

debug:
  enabled: false